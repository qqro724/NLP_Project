# -*- coding: utf-8 -*-
"""asr_whisper_korea_colab.ipynb의 사본의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QrAObYLSG8X3BgloXZFzF9Pp9MlpozlS

# 음성인식 실습

220922, by *wygo*

- OpenAI의 , 27sec 음성을 12sec 이내에 음성인식 완료. 성능도 괜찮음

- [실습코드](https://github.com/airobotlab/asr_whisper_korea_colab)
- [blog](https://openai.com/blog/whisper/)
- [paper](https://cdn.openai.com/papers/whisper.pdf)
- [code](https://github.com/openai/whisper)

![model architecture](https://github.com/openai/whisper/raw/main/approach.png)

# 0. 음성인식 모델 셋팅
### openai의 whisper 모델
"""

## install
!pip install git+https://github.com/openai/whisper.git

# Commented out IPython magic to ensure Python compatibility.
## import
import whisper
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import librosa
import librosa.display
from pprint import pprint

"""# 1. audio 데이터

# - [음성다운사이트](https://xn--2n1bk9rtmh26jp7fdva.com/3541)
"""

# ## 1.1. 음성파일 업로드
# from google.colab import files
# file_uploaded = files.upload()
# input_audio_path = list(file_uploaded.keys())[0]
# print('file name: %s'%input_audio_path)

## 1.2. 예시 음성파일 사용
!git clone https://github.com/airobotlab/asr_whisper_korea_colab.git
!mv asr_whisper_korea_colab/sample_audio/* ./

input_audio_path = 'audio1.mp3'  # 한국어1
# input_audio_path = 'audio2.wav'  # 한국어2
# input_audio_path = 'audio3.wav'  # 한국어3
# input_audio_path = 'audio_jp.mp3'  # 일본어, 14sec

"""# 2. audio 분석"""

## analyze audio with librosa
x, sr = librosa.load(input_audio_path)
fig, ax = plt.subplots(figsize=(15, 5), nrows=2, ncols=1, sharex=False)

librosa.display.waveplot(x, sr=sr, ax=ax[0], x_axis='time')
ax[0].set(title='Linear-frequency power spectrogram')
ax[0].label_outer()

D = librosa.amplitude_to_db(np.abs(librosa.stft(x, hop_length=160)), ref=np.max)
tmp = librosa.display.specshow(D, y_axis='mel', sr=sr, hop_length=160, x_axis='time', ax=ax[1])
ax[1].set(title='Log-frequency power spectrogram')
ax[1].label_outer()
# fig.colorbar(tmp, ax=ax[1], format="%+2.f dB")
plt.plot()

import IPython.display as ipd
ipd.Audio(input_audio_path) # load a local WAV file

"""# 3. 음성인식 모델 준비"""

## load model
model = whisper.load_model("small")
print(f"Model is {'multilingual' if model.is_multilingual else 'English-only'} "
      f"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.")

## load audio and pad/trim it to fit 30
# input_audio_path = 'audio1.mp3'
audio_raw = whisper.load_audio(input_audio_path)
audio = whisper.pad_or_trim(audio_raw)

print('%s -> %s'%(audio.shape,audio.shape))
print('max value: %s'%audio.max())
print('min value: %s'%audio.min())

## make log-Mel spectrogram and move to the same device as the model
melspectrogram = whisper.log_mel_spectrogram(audio).to(model.device)

print(melspectrogram.shape)
print(melspectrogram[0])
print('max value: %s'%audio.max())
print('min value: %s'%audio.min())

# plot log_melspectogram
plt.figure(figsize=(10, 4))
librosa.display.specshow(melspectrogram.cpu().numpy(), y_axis='mel', x_axis='time', sr=16000, hop_length=160)
plt.colorbar(format='%+2.0f dB')
plt.title('Mel-Spectrogram')
plt.tight_layout()
plt.savefig('Mel-Spectrogram example.png')
plt.show()

"""## 4. 음성인식!"""

## run ASR
# decode the audio
options = whisper.DecodingOptions()
result = whisper.decode(model, melspectrogram, options)

# print the recognized text
print(result.text)

