# -*- coding: utf-8 -*-
"""TF-IDF 활용 (2)_1002.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fGeDuuZlfXmwho196O1QzoCz2mWBde8M
"""

!pip install googletrans==4.0.0rc1

!pip install konlpy

!pip install Okt

import pandas as pd

import os
import re  # 정규 표현식을 사용하기 위해 추가
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt

# 데이터 파일 디렉토리 경로 설정
data_dir = '/content/drive/MyDrive/Symptom'

# 데이터 파일 이름 리스트
data_file_names = [
    "combined_cold.csv",
    "combined_eyes.csv",
    "combined_stomach.csv"
]

file_cold = "combined_cold.csv",
file_eyes = "combined_eyes.csv"
file_stomach= "combined_stomach.csv"

# 데이터 파일 변수와 진단 임계값 딕셔너리 설정
data_files = {
    '감기': file_cold,
    '위염': file_stomach,
    '결막염': file_eyes,
}

thresholds = {
    '감기': 0.5,
    '위염': 0.5,
    '결막염': 0.5,
}

# 데이터 파일의 전체 경로 생성
data_file_paths = [os.path.join(data_dir, filename) for filename in data_file_names]

# 진단 결과 딕셔너리 초기화
diagnosis_results = {}

# 입력 증상 텍스트 (사용자로부터 입력을 받거나 직접 설정)
input_symptom = '위궤양 증상'  # 감기 관련 증상을 입력하세요.

# 진단 함수 정의
def preprocess_text(text):
    # 한글과 공백을 제외한 모든 문자 제거
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\s]', '', text)

    # 불용어 처리 및 형태소 분석
    okt = Okt()
    words = okt.pos(text, stem=True)
    words = [word for word, pos in words if pos in ['Noun', 'Adjective']]

    # 공백으로 구분된 단어들을 다시 결합
    text = ' '.join(words)

    return text

def preprocess_file(file_path):
    # 파일 읽어오기
    df = pd.read_csv(file_path, encoding="utf-8")

    # 텍스트 데이터 전처리
    df['combined_text'] = df['Answer'] + df['Question']
    df['combined_text'] = df['combined_text'].apply(preprocess_text)

    return df

def diagnose_symptom(input_symptom, df, threshold):
    # 입력 증상 전처리
    input_symptom = preprocess_text(input_symptom)

    # TF-IDF 벡터화
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])

    # 입력 증상과 리뷰 간의 코사인 유사도 계산
    similarities = cosine_similarity(tfidf_vectorizer.transform([input_symptom]), tfidf_matrix)

    # 가장 높은 유사도와 관련된 진단 찾기
    max_similarity = similarities.max()
    matched_diagnosis = None

    if max_similarity >= threshold:
        matched_diagnosis = df.loc[similarities.argmax(), 'Question']

    return matched_diagnosis if matched_diagnosis else "진단을 확인할 수 없습니다."

# 진단 수행 및 결과 저장
for data_file_path in data_file_paths:
    diagnosis_name = os.path.splitext(os.path.basename(data_file_path))[0]
    threshold = thresholds.get(diagnosis_name, 0.5)
    df = preprocess_file(data_file_path)
    matched_diagnosis = diagnose_symptom(input_symptom, df, threshold)
    diagnosis_results[diagnosis_name] = matched_diagnosis

# 진단 결과 출력
for diagnosis, result in diagnosis_results.items():
    print(f"진단 결과 ({diagnosis}): {result}")

"""

---

lstm 으로 질병 분류"""

import os
import re  # 정규 표현식을 사용하기 위해 추가
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt

# 데이터 파일 디렉토리 경로 설정
data_dir = '/content/Symptom'

# 데이터 파일 이름 리스트
data_file_names = [
    "combined_cold.csv",
    "combined_eyes.csv",
    "combined_stomach.csv"
]

file_cold = "combined_cold.csv",
file_eyes = "combined_eyes.csv"
file_stomach= "combined_stomach.csv"

# 데이터 파일 변수와 진단 임계값 딕셔너리 설정
data_files = {
    '감기': file_cold,
    '위염': file_stomach,
    '결막염': file_eyes,
}

thresholds = {
    '감기': 0.5,
    '위염': 0.5,
    '결막염': 0.5,
}

# 데이터 파일의 전체 경로 생성
data_file_paths = [os.path.join(data_dir, filename) for filename in data_file_names]

# 진단 결과 딕셔너리 초기화
diagnosis_results = {}

# 입력 증상 텍스트 (사용자로부터 입력을 받거나 직접 설정)
input_symptom = '기침, 가래 , 콧물 증상'  # 감기 관련 증상을 입력하세요.

# 진단 함수 정의
def preprocess_text(text):
    # 한글과 공백을 제외한 모든 문자 제거
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\s]', '', text)

    # 불용어 처리 및 형태소 분석
    okt = Okt()
    words = okt.pos(text, stem=True)
    words = [word for word, pos in words if pos in ['Noun', 'Adjective']]

    # 공백으로 구분된 단어들을 다시 결합
    text = ' '.join(words)

    return text

def preprocess_file(file_path):
    # 파일 읽어오기
    df = pd.read_csv(file_path, encoding="utf-8")

    # 텍스트 데이터 전처리
    df['combined_text'] = df['Answer'] + df['Question']
    df['combined_text'] = df['combined_text'].apply(preprocess_text)

    return df

def diagnose_symptom(input_symptom, df, threshold):
    # 입력 증상 전처리
    input_symptom = preprocess_text(input_symptom)

    # TF-IDF 벡터화
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(df['combined_text'])

    # 입력 증상과 리뷰 간의 코사인 유사도 계산
    similarities = cosine_similarity(tfidf_vectorizer.transform([input_symptom]), tfidf_matrix)

    # 가장 높은 유사도와 관련된 진단 찾기
    max_similarity = similarities.max()
    matched_diagnosis = None

    if max_similarity >= threshold:
        matched_diagnosis = df.loc[similarities.argmax(), 'Question']

    return matched_diagnosis if matched_diagnosis else "진단을 확인할 수 없습니다."

# 진단 수행 및 결과 저장
for data_file_path in data_file_paths:
    diagnosis_name = os.path.splitext(os.path.basename(data_file_path))[0]
    threshold = thresholds.get(diagnosis_name, 0.5)
    df = preprocess_file(data_file_path)
    matched_diagnosis = diagnose_symptom(input_symptom, df, threshold)
    diagnosis_results[diagnosis_name] = matched_diagnosis

# 진단 결과 출력
for diagnosis, result in diagnosis_results.items():
    print(f"진단 결과 ({diagnosis}): {result}")

import os
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input
from sklearn.model_selection import train_test_split
import numpy as np

# 데이터 파일 디렉토리 경로 설정
data_dir = '/content/drive/MyDrive/Symptom'

# 데이터 파일 이름 리스트
data_file_names = [
    "combined_cold.csv",
    "combined_eyes.csv",
    "combined_stomach.csv"
]

file_cold = "combined_cold.csv"
file_eyes = "combined_eyes.csv"
file_stomach = "combined_stomach.csv"

# 데이터 파일 변수와 진단 임계값 딕셔너리 설정
data_files = {
    '감기': file_cold,
    '위염': file_stomach,
    '결막염': file_eyes,
}

thresholds = {
    '감기': 0.7,
    '위염': 0.7,
    '결막염': 0.7,
}

# 데이터 파일의 전체 경로 생성
data_file_paths = [os.path.join(data_dir, filename) for filename in data_file_names]

# 입력 증상 텍스트 (사용자로부터 입력을 받거나 직접 설정)
input_symptom = '위궤양 증상'  # 감기 관련 증상을 입력하세요.

# 진단 함수 정의
def preprocess_text(text):
    # 한글과 공백을 제외한 모든 문자 제거
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\s]', '', text)

    # 불용어 처리 및 형태소 분석
    okt = Okt()
    words = okt.pos(text, stem=True)
    words = [word for word, pos in words if pos in ['Noun', 'Adjective']]

    # 공백으로 구분된 단어들을 다시 결합
    text = ' '.join(words)

    return text

def preprocess_file(file_path):
    # 파일 읽어오기
    df = pd.read_csv(file_path, encoding="utf-8")

    # 텍스트 데이터 전처리
    df['combined_text'] = df['Answer'] + df['Question']
    df['combined_text'] = df['combined_text'].apply(preprocess_text)

    return df

# TF-IDF 모델 초기화 및 fitting
tfidf_vectorizer = TfidfVectorizer()
# 모든 데이터 파일에 대해 TF-IDF 모델을 fitting
all_texts = []
for data_file_path in data_file_paths:
    df = preprocess_file(data_file_path)
    all_texts.extend(df['combined_text'])
tfidf_vectorizer.fit(all_texts)

# 진단 결과 딕셔너리 초기화
diagnosis_results = {}

# 진단 수행 및 결과 저장
for data_file_path in data_file_paths:
    diagnosis_name = os.path.splitext(os.path.basename(data_file_path))[0]
    threshold = thresholds.get(diagnosis_name, 0.5)
    df = preprocess_file(data_file_path)

    # TF-IDF로 진단명을 찾기
    input_symptom_tfidf = tfidf_vectorizer.transform([input_symptom])
    tfidf_matrix = tfidf_vectorizer.transform(df['combined_text'])
    similarities = cosine_similarity(input_symptom_tfidf, tfidf_matrix)
    max_similarity = similarities.max()
    matched_diagnosis_tfidf = None

    if max_similarity >= threshold:
        matched_diagnosis_tfidf = df.loc[similarities.argmax(), 'Question']

    # MLP 모델을 사용한 진단 예측
    X = tfidf_matrix
    y = df['Answer'].apply(lambda x: 1 if x == input_symptom else 0)
    X_train, X_val, y_train, y_val = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)

    model = keras.Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    predictions = model.predict(X_val)
    predicted_diagnosis_mlp = df.loc[np.argmax(predictions), 'Question']

    # TF-IDF와 MLP 결과를 앙상블
    if matched_diagnosis_tfidf and predicted_diagnosis_mlp:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF) & " + predicted_diagnosis_mlp + " (MLP)"
    elif matched_diagnosis_tfidf:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    elif predicted_diagnosis_mlp:
        final_diagnosis = predicted_diagnosis_mlp + " (MLP)"
    else:
        final_diagnosis = "진단을 확인할 수 없습니다."

    diagnosis_results[diagnosis_name] = final_diagnosis

# 결과 출력
for diagnosis, result in diagnosis_results.items():
    print(f"진단 결과 ({diagnosis}): {result}")

import os
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input
from sklearn.model_selection import train_test_split
import numpy as np

# 데이터 파일 디렉토리 경로 설정
data_dir = '/content/drive/MyDrive/Symptom'

# 데이터 파일 이름 리스트
data_file_names = [
    "combined_cold.csv",
    "combined_eyes.csv",
    "combined_stomach.csv"
]

file_cold = "combined_cold.csv"
file_eyes = "combined_eyes.csv"
file_stomach = "combined_stomach.csv"

# 데이터 파일 변수와 진단 임계값 딕셔너리 설정
data_files = {
    '감기': file_cold,
    '위염': file_stomach,
    '결막염': file_eyes,
}

thresholds = {
    '감기': 0.7,
    '위염': 0.7,
    '결막염': 0.7,
}

# 데이터 파일의 전체 경로 생성
data_file_paths = [os.path.join(data_dir, filename) for filename in data_file_names]

# 입력 증상 텍스트 (사용자로부터 입력을 받거나 직접 설정)
input_symptom = '기침이 많이 나'  # 감기 관련 증상을 입력하세요.

# 진단 함수 정의
def preprocess_text(text):
    # 한글과 공백을 제외한 모든 문자 제거
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\s]', '', text)

    # 불용어 처리 및 형태소 분석
    okt = Okt()
    words = okt.pos(text, stem=True)
    words = [word for word, pos in words if pos in ['Noun', 'Adjective']]

    # 공백으로 구분된 단어들을 다시 결합
    text = ' '.join(words)

    return text

def preprocess_file(file_path):
    # 파일 읽어오기
    df = pd.read_csv(file_path, encoding="utf-8")

    # 텍스트 데이터 전처리
    df['combined_text'] = df['Answer'] + df['Question']
    df['combined_text'] = df['combined_text'].apply(preprocess_text)

    return df

# TF-IDF 모델 초기화 및 fitting
tfidf_vectorizer = TfidfVectorizer()
# 모든 데이터 파일에 대해 TF-IDF 모델을 fitting
all_texts = []
for data_file_path in data_file_paths:
    df = preprocess_file(data_file_path)
    all_texts.extend(df['combined_text'])
tfidf_vectorizer.fit(all_texts)

# 진단 결과 딕셔너리 초기화
diagnosis_results = {}

# 진단 수행 및 결과 저장
for data_file_path in data_file_paths:
    diagnosis_name = os.path.splitext(os.path.basename(data_file_path))[0]
    threshold = thresholds.get(diagnosis_name, 0.5)
    df = preprocess_file(data_file_path)

    # TF-IDF로 진단명을 찾기
    input_symptom_tfidf = tfidf_vectorizer.transform([input_symptom])
    tfidf_matrix = tfidf_vectorizer.transform(df['combined_text'])
    similarities = cosine_similarity(input_symptom_tfidf, tfidf_matrix)
    max_similarity = similarities.max()
    matched_diagnosis_tfidf = None

    if max_similarity >= threshold:
        matched_diagnosis_tfidf = df.loc[similarities.argmax(), 'Question']

    # MLP 모델을 사용한 진단 예측
    X = tfidf_matrix
    y = df['Answer'].apply(lambda x: 1 if x == input_symptom else 0)
    X_train, X_val, y_train, y_val = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)

    model = keras.Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(128, activation='relu'),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    predictions = model.predict(X_val)
    predicted_diagnosis_mlp = df.loc[np.argmax(predictions), 'Question']

    # TF-IDF와 MLP 결과 중 가장 높은 유사도 또는 확률을 가진 진단을 선택
    if max_similarity >= threshold and np.max(predictions) >= 0.5:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    elif np.max(predictions) >= 0.5:
        final_diagnosis = predicted_diagnosis_mlp + " (MLP)"
    elif max_similarity >= threshold:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    else:
        final_diagnosis = "진단을 확인할 수 없습니다."

    diagnosis_results[diagnosis_name] = final_diagnosis

# 결과 출력
for diagnosis, result in diagnosis_results.items():
    print(f"진단 결과 ({diagnosis}): {result}")

import os
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input, Dropout
from sklearn.model_selection import train_test_split
import numpy as np

# 데이터 파일 디렉토리 경로 설정
data_dir = '/content/drive/MyDrive/Symptom'

# 데이터 파일 이름 리스트
data_file_names = [
    "combined_cold.csv",
    "combined_eyes.csv",
    "combined_stomach.csv"
]

file_cold = "combined_cold.csv"
file_eyes = "combined_eyes.csv"
file_stomach = "combined_stomach.csv"

# 데이터 파일 변수와 진단 임계값 딕셔너리 설정
data_files = {
    '감기': file_cold,
    '위염': file_stomach,
    '결막염': file_eyes,
}

thresholds = {
    '감기': 0.7,
    '위염': 0.7,
    '결막염': 0.7,
}

# 데이터 파일의 전체 경로 생성
data_file_paths = [os.path.join(data_dir, filename) for filename in data_file_names]

# 입력 증상 텍스트 (사용자로부터 입력을 받거나 직접 설정)
input_symptom = '위궤양 증상'  # 감기 관련 증상을 입력하세요.

# 진단 함수 정의
def preprocess_text(text):
    # 한글과 공백을 제외한 모든 문자 제거
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\s]', '', text)

    # 불용어 처리 및 형태소 분석
    okt = Okt()
    words = okt.pos(text, stem=True)
    words = [word for word, pos in words if pos in ['Noun', 'Adjective']]

    # 공백으로 구분된 단어들을 다시 결합
    text = ' '.join(words)

    return text

def preprocess_file(file_path):
    # 파일 읽어오기
    df = pd.read_csv(file_path, encoding="utf-8")

    # 텍스트 데이터 전처리
    df['combined_text'] = df['Answer'] + df['Question']
    df['combined_text'] = df['combined_text'].apply(preprocess_text)

    return df

# TF-IDF 모델 초기화 및 fitting
tfidf_vectorizer = TfidfVectorizer()
# 모든 데이터 파일에 대해 TF-IDF 모델을 fitting
all_texts = []
for data_file_path in data_file_paths:
    df = preprocess_file(data_file_path)
    all_texts.extend(df['combined_text'])
tfidf_vectorizer.fit(all_texts)

# 진단 결과 딕셔너리 초기화
diagnosis_results = {}

# MLP 모델 정의 및 훈련
def train_mlp_model(X_train, y_train):
    model = keras.Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(128, activation='relu'),
        Dropout(0.5),  # 드롭아웃 레이어 추가 (0.5는 비활성화될 뉴런의 비율입니다)
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)  # verbose=1로 설정하여 훈련 과정 출력

    return model

# 진단 수행 및 결과 저장
for data_file_path in data_file_paths:
    diagnosis_name = os.path.splitext(os.path.basename(data_file_path))[0]
    threshold = thresholds.get(diagnosis_name, 0.5)
    df = preprocess_file(data_file_path)

    # TF-IDF로 진단명을 찾기
    input_symptom_tfidf = tfidf_vectorizer.transform([input_symptom])
    tfidf_matrix = tfidf_vectorizer.transform(df['combined_text'])
    similarities = cosine_similarity(input_symptom_tfidf, tfidf_matrix)
    max_similarity = similarities.max()
    matched_diagnosis_tfidf = None

    if max_similarity >= threshold:
        matched_diagnosis_tfidf = df.loc[similarities.argmax(), 'Question']

    # MLP 모델을 사용한 진단 예측
    X = tfidf_matrix
    y = df['Answer'].apply(lambda x: 1 if x == input_symptom else 0)
    X_train, X_val, y_train, y_val = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)

    model = train_mlp_model(X_train, y_train)

    # 테스트 데이터를 사용하여 정확도 계산
    _, accuracy = model.evaluate(X_val, y_val)

    # 정확도가 0.5 이상인 경우만 해당 모델로 진단
    if max_similarity >= threshold and accuracy >= 0.5:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    elif accuracy >= 0.5:
        final_diagnosis = "MLP 모델로 진단"
    elif max_similarity >= threshold:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    else:
        final_diagnosis = "진단을 확인할 수 없습니다."

    diagnosis_results[diagnosis_name] = final_diagnosis

# 결과 출력
for diagnosis, result in diagnosis_results.items():
    print(f"진단 결과 ({diagnosis}): {result}")

import os
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input, Dropout
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# 데이터 파일 디렉토리 경로 설정
data_dir = '/content/drive/MyDrive/Symptom'

# 데이터 파일 이름 리스트
data_file_names = [
    "combined_cold.csv",
    "combined_eyes.csv",
    "combined_stomach.csv"
]

file_cold = "combined_cold.csv"
file_eyes = "combined_eyes.csv"
file_stomach = "combined_stomach.csv"

# 데이터 파일 변수와 진단 임계값 딕셔너리 설정
data_files = {
    '감기': file_cold,
    '위염': file_stomach,
    '결막염': file_eyes,
}

thresholds = {
    '감기': 0.7,
    '위염': 0.7,
    '결막염': 0.7,
}

# 데이터 파일의 전체 경로 생성
data_file_paths = [os.path.join(data_dir, filename) for filename in data_file_names]

# 입력 증상 텍스트 (사용자로부터 입력을 받거나 직접 설정)
input_symptom = '위궤양 증상'  # 감기 관련 증상을 입력하세요.

# 진단 함수 정의
def preprocess_text(text):
    # 한글과 공백을 제외한 모든 문자 제거
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\s]', '', text)

    # 불용어 처리 및 형태소 분석
    okt = Okt()
    words = okt.pos(text, stem=True)
    words = [word for word, pos in words if pos in ['Noun', 'Adjective']]

    # 공백으로 구분된 단어들을 다시 결합
    text = ' '.join(words)

    return text

def preprocess_file(file_path):
    # 파일 읽어오기
    df = pd.read_csv(file_path, encoding="utf-8")

    # 텍스트 데이터 전처리
    df['combined_text'] = df['Answer'] + df['Question']
    df['combined_text'] = df['combined_text'].apply(preprocess_text)

    return df

# TF-IDF 모델 초기화 및 fitting
tfidf_vectorizer = TfidfVectorizer()
# 모든 데이터 파일에 대해 TF-IDF 모델을 fitting
all_texts = []
for data_file_path in data_file_paths:
    df = preprocess_file(data_file_path)
    all_texts.extend(df['combined_text'])
tfidf_vectorizer.fit(all_texts)

# 진단 결과 딕셔너리 초기화
diagnosis_results = {}

# MLP 모델 정의 및 훈련 (과적합 완화)
def train_regularized_mlp_model(X_train, y_train):
    model = keras.Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        Dropout(0.5),
        Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_split=0.2)

    return model, history

# 진단 수행 및 결과 저장 (가중치 규제 추가)
for data_file_path in data_file_paths:
    diagnosis_name = os.path.splitext(os.path.basename(data_file_path))[0]
    threshold = thresholds.get(diagnosis_name, 0.5)
    df = preprocess_file(data_file_path)

    # TF-IDF로 진단명을 찾기
    input_symptom_tfidf = tfidf_vectorizer.transform([input_symptom])
    tfidf_matrix = tfidf_vectorizer.transform(df['combined_text'])
    similarities = cosine_similarity(input_symptom_tfidf, tfidf_matrix)
    max_similarity = similarities.max()
    matched_diagnosis_tfidf = None

    if max_similarity >= threshold:
        matched_diagnosis_tfidf = df.loc[similarities.argmax(), 'Question']

    # MLP 모델을 사용한 진단 예측 (가중치 규제를 적용한 모델 사용)
    X = tfidf_matrix
    y = df['Answer'].apply(lambda x: 1 if x == input_symptom else 0)
    X_train, X_val, y_train, y_val = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)

    model, history = train_regularized_mlp_model(X_train, y_train)

    # 테스트 데이터를 사용하여 정확도 계산
    _, accuracy = model.evaluate(X_val, y_val)

    # 정확도가 0.5 이상인 경우만 해당 모델로 진단
    if max_similarity >= threshold and accuracy >= 0.5:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    elif accuracy >= 0.5:
        final_diagnosis = "MLP 모델로 진단"
    elif max_similarity >= threshold:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    else:
        final_diagnosis = "진단을 확인할 수 없습니다."

    diagnosis_results[diagnosis_name] = final_diagnosis

    # 모델 훈련 과정 시각화
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Accuracy vs. Epochs')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss vs. Epochs')

    plt.show()

# 결과 출력
for diagnosis, result in diagnosis_results.items():
    print(f"진단 결과 ({diagnosis}): {result}")

import os
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input, Dropout
from sklearn.model_selection import train_test_split
import numpy as np

# 데이터 파일 디렉토리 경로 설정
data_dir = '/content/drive/MyDrive/Symptom'

# 데이터 파일 이름 리스트
data_file_names = [
    "combined_cold.csv",
    "combined_eyes.csv",
    "combined_stomach.csv"
]

file_cold = "combined_cold.csv"
file_eyes = "combined_eyes.csv"
file_stomach = "combined_stomach.csv"

# 데이터 파일 변수와 진단 임계값 딕셔너리 설정
data_files = {
    '감기': file_cold,
    '위염': file_stomach,
    '결막염': file_eyes,
}

thresholds = {
    '감기': 0.7,
    '위염': 0.7,
    '결막염': 0.7,
}

# 데이터 파일의 전체 경로 생성
data_file_paths = [os.path.join(data_dir, filename) for filename in data_file_names]

# 입력 증상 텍스트 (사용자로부터 입력을 받거나 직접 설정)
input_symptom = '위궤양 증상'  # 감기 관련 증상을 입력하세요.

# 진단 함수 정의
def preprocess_text(text):
    # 한글과 공백을 제외한 모든 문자 제거
    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z\s]', '', text)

    # 불용어 처리 및 형태소 분석
    okt = Okt()
    words = okt.pos(text, stem=True)
    words = [word for word, pos in words if pos in ['Noun', 'Adjective']]

    # 공백으로 구분된 단어들을 다시 결합
    text = ' '.join(words)

    return text

def preprocess_file(file_path):
    # 파일 읽어오기
    df = pd.read_csv(file_path, encoding="utf-8")

    # 텍스트 데이터 전처리
    df['combined_text'] = df['Answer'] + df['Question']
    df['combined_text'] = df['combined_text'].apply(preprocess_text)

    return df

# TF-IDF 모델 초기화 및 fitting
tfidf_vectorizer = TfidfVectorizer()
# 모든 데이터 파일에 대해 TF-IDF 모델을 fitting
all_texts = []
for data_file_path in data_file_paths:
    df = preprocess_file(data_file_path)
    all_texts.extend(df['combined_text'])
tfidf_vectorizer.fit(all_texts)

# 진단 결과 딕셔너리 초기화
diagnosis_results = {}

# MLP 모델 정의 및 훈련
def train_mlp_model(X_train, y_train):
    model = keras.Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(128, activation='relu'),
        Dropout(0.5),  # 드롭아웃 레이어 추가 (0.5는 비활성화될 뉴런의 비율입니다)
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)  # verbose=1로 설정하여 훈련 과정 출력

    return model

# 진단 수행 및 결과 저장
for data_file_path in data_file_paths:
    diagnosis_name = os.path.splitext(os.path.basename(data_file_path))[0]
    threshold = thresholds.get(diagnosis_name, 0.5)
    df = preprocess_file(data_file_path)

    # TF-IDF로 진단명을 찾기
    input_symptom_tfidf = tfidf_vectorizer.transform([input_symptom])
    tfidf_matrix = tfidf_vectorizer.transform(df['combined_text'])
    similarities = cosine_similarity(input_symptom_tfidf, tfidf_matrix)
    max_similarity = similarities.max()
    matched_diagnosis_tfidf = None

    if max_similarity >= threshold:
        matched_diagnosis_tfidf = df.loc[similarities.argmax(), 'Question']

    # MLP 모델을 사용한 진단 예측
    X = tfidf_matrix
    y = df['Answer'].apply(lambda x: 1 if x == input_symptom else 0)
    X_train, X_val, y_train, y_val = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)

    model = train_mlp_model(X_train, y_train)

    # 테스트 데이터를 사용하여 정확도 계산
    _, accuracy = model.evaluate(X_val, y_val)

    # 정확도가 0.5 이상인 경우만 해당 모델로 진단
    if max_similarity >= threshold and accuracy >= 0.5:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    elif accuracy >= 0.5:
        final_diagnosis = diagnosis_name  # 각각의 진단명으로 증상 검출
    elif max_similarity >= threshold:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    else:
        final_diagnosis = "진단을 확인할 수 없습니다."

    diagnosis_results[diagnosis_name] = final_diagnosis

# 결과 출력
for diagnosis, result in diagnosis_results.items():
    print(f"진단 결과 ({diagnosis}): {result}")

import os
import re
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Input, Dropout
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# ... (이전 코드 부분은 그대로 유지)

# 정확도 저장 변수 초기화
accuracies = {}

# MLP 모델 정의 및 훈련
def train_mlp_model(X_train, y_train, X_val, y_val):
    model = keras.Sequential([
        Input(shape=(X_train.shape[1],)),
        Dense(128, activation='relu'),
        Dropout(0.5),  # 드롭아웃 레이어 추가 (0.5는 비활성화될 뉴런의 비율입니다)
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=1)

    # 정확도 그래프 생성
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    return model

# 진단 수행 및 결과 저장
for data_file_path in data_file_paths:
    diagnosis_name = os.path.splitext(os.path.basename(data_file_path))[0]
    threshold = thresholds.get(diagnosis_name, 0.5)
    df = preprocess_file(data_file_path)

    # TF-IDF로 진단명을 찾기
    input_symptom_tfidf = tfidf_vectorizer.transform([input_symptom])
    tfidf_matrix = tfidf_vectorizer.transform(df['combined_text'])
    similarities = cosine_similarity(input_symptom_tfidf, tfidf_matrix)
    max_similarity = similarities.max()
    matched_diagnosis_tfidf = None

    if max_similarity >= threshold:
        matched_diagnosis_tfidf = df.loc[similarities.argmax(), 'Question']

    # MLP 모델을 사용한 진단 예측
    X = tfidf_matrix
    y = df['Answer'].apply(lambda x: 1 if x == input_symptom else 0)
    X_train, X_val, y_train, y_val = train_test_split(X.toarray(), y, test_size=0.2, random_state=42)

    model = train_mlp_model(X_train, y_train, X_val, y_val)

    # 테스트 데이터를 사용하여 정확도 계산
    _, accuracy = model.evaluate(X_val, y_val)

    # 정확도가 0.5 이상인 경우만 해당 모델로 진단
    if max_similarity >= threshold and accuracy >= 0.5:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    elif accuracy >= 0.5:
        final_diagnosis = diagnosis_name  # 각각의 진단명으로 증상 검출
    elif max_similarity >= threshold:
        final_diagnosis = matched_diagnosis_tfidf + " (TF-IDF)"
    else:
        final_diagnosis = "진단을 확인할 수 없습니다."

    diagnosis_results[diagnosis_name] = final_diagnosis
    accuracies[diagnosis_name] = accuracy  # 정확도 저장

# 결과 출력
for diagnosis, result in diagnosis_results.items():
    print(f"진단 결과 ({diagnosis}): {result}")
    print(f"모델 정확도 ({diagnosis}): {accuracies[diagnosis]:.4f}")